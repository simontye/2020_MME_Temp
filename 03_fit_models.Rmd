---
title: "fit models"
author: "fishkill friends"
date: "2020/11/10"
output: github_document
---

### Background

Models are fit to the training data from `02_prepare_models` via four primary model families listed below. This is the same workflow as Till et al. (2019) except that I switched the file conversion from tibble to dataframe because of parsing errors (on my end).

1. Logistic regression
2. Ridge regression
3. Lasso regression
4. Logistic regression with random effects

### Step 1: Load packages and data

```{r: Load packages and data}
# Load packages
library(tidyverse)
library(rsample)
library(caret)
library(glmnet)
library(Matrix)
library(e1071)
library(pROC)
library(PRROC) 
library(glmnetUtils)
library(doParallel)
library(lme4)
library(optimx)

# Reset global environment
rm(list = ls())

# Set working directory
setwd("/Users/simontye/Documents/Research/Projects/MME_Temp/2020_MME_Temp/data/models")

# Load data
training    <- read.csv(file = "training.csv", head = TRUE, sep = ",")
testing     <- read.csv(file = "testing.csv",  head = TRUE, sep = ",")

# Reformat training data
training <- training %>%
  mutate(site_id      = as.character(site_id),
         year         = as.character(year),
         month        = as.character(month),
         season       = as.character(season),
         summerkill   = as.factor(summerkill),
         ice_duration = as.double(ice_duration),
         population   = as.numeric(population),
         state        = as.character(state),
         l3_code      = as.character(l3_code))
# Reformat testing data
testing <- testing %>%
  mutate(site_id      = as.character(site_id),
         year         = as.character(year),
         month        = as.character(month),
         season       = as.character(season),
         summerkill   = as.factor(summerkill),
         ice_duration = as.double(ice_duration),
         population   = as.numeric(population),
         state        = as.character(state),
         l3_code      = as.character(l3_code))
```

### Step 2: Specify variable sets

New models with mean_z temps

```{r: New generalized model}
# New generalized model for water temperature (state)
w1 <- summerkill ~ long + lat + season + population +
  mean_surf_z + water_pca + state

# New generalized model for air temperature (state)
a1 <- summerkill ~ long + lat + season + population +
  mean_air_z + air_pca + state
```

### Step 3: Set up parameters for lambda selection for ridge and lasso regressions

Black magic.

```{r: Set up parameters for ridge and lasso models}
# No downsampling
control_logloss <- trainControl(method = "repeatedcv",
                                number = 5,
                                repeats = 5,
                                summaryFunction = mnLogLoss,
                                classProbs = TRUE)

# Downsampling
control_logloss_ds <- trainControl(method = "repeatedcv",
                                   number = 5,
                                   repeats = 5,
                                   summaryFunction = mnLogLoss,
                                   classProbs = TRUE,
                                   sampling = "down")
```

### Step 4: Logistic regressions

New generalized models

```{r: Simple logistic regression}
# Set seed
set.seed(736)

# New generalized model for air temperature
logistic_a1 <- glm(a1, training, family = "binomial")

# Stepwise model comparisons by AIC
#logistic_a1 <- step(logistic_a1)

# New generalized model for water temperatures
logistic_w1 <- glm(w1, training, family = "binomial")

# Stepwise model comparisons by AIC
#logistic_w1 <- step(logistic_w1)

# Save models 
write_rds(logistic_a1, "assessment/logistic_a1.rds")
write_rds(logistic_w1, "assessment/logistic_w1.rds")

# Remove models
rm(logistic_a1, logistic_w1)
```

### Step 5: Lasso regressions

This fits LASSO models with many differenct lambdas, utilizing a 5-fold CV scheme, repeated five times, on the training data.

```{r: First lasso regression w/o downsampling}
# Black magic
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

# Black magic
par_grid <-  expand.grid(alpha = 1, lambda = seq(1.5e-6, 1e-5, length.out = 6))

# Set seed
set.seed(685)

# Lasso model without downsampling for air temperature
lasso_a1_logloss <- train(a1,
                          data = training, 
                          method = "glmnet", 
                          metric = "mnLogLoss",
                          tuneGrid = par_grid,
                          trControl = control_logloss)

# Lasso model without downsampling for water temperature
lasso_w1_logloss <- train(w1,
                          data = training, 
                          method = "glmnet", 
                          metric = "mnLogLoss",
                          tuneGrid = par_grid,
                          trControl = control_logloss)



#
cols_reg = c('pce', 'pop', 'psavert', 'uempmed', 'unemploy')

dummies <- dummyVars(unemploy ~ ., data = dat[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))


# Black magic
stopCluster(cl)

# Save models
write_rds(lasso_a1_logloss, "assessment/lasso_a1_logloss.rds")
write_rds(lasso_w1_logloss, "assessment/lasso_w1_logloss.rds")

# Remove models
rm(lasso_a1_logloss, lasso_w1_logloss)
```

New models with logloss and downsampling.

```{r: First lasso regression w/ downsampling}
# Black magic
cl <- makePSOCKcluster(4)

# Black magic
registerDoParallel(cl)

# Black magic
par_grid <-  expand.grid(alpha = 1, lambda = 10^seq(-3.5, -1.3, length = 8))

# Set seed
set.seed(336)

# Lasso model with downsampling air temperature
lasso_a1_logloss_downsampled <- train(a1, 
                                      data = training, 
                                      method = "glmnet", 
                                      metric = "logLoss",
                                      tuneGrid = par_grid,
                                      trControl = control_logloss_ds)

# Lasso model with downsampling for water temperature
lasso_w1_logloss_downsampled <- train(w1, 
                                      data = training, 
                                      method = "glmnet", 
                                      metric = "logLoss",
                                      tuneGrid = par_grid,
                                      trControl = control_logloss_ds)

# Black magic
stopCluster(cl)

# Save models
write_rds(lasso_a1_logloss_ds, "assessment/lasso_a1_logloss_ds.rds")
write_rds(lasso_w1_logloss_ds, "assessment/lasso_w1_logloss_ds.rds")

# Remove models
rm(lasso_a1_logloss_ds, lasso_w1_logloss_ds)
```

### Step 6: Ridge logistic regressions

New models with logloss and without downsampling.

```{r: First ridge regression w/o downsampling}
# Black magic
par_grid <-  expand.grid(alpha = 0,
                        lambda = seq(3.5e-7, 1.5e-5,
                                     length.out = 6))

# Black magic
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

# Set seed
set.seed(475)

# Ridge model without downsampling for water temperature
ridge_w1_logloss <- train(w1, 
                          data = training, 
                          method = "glmnet",
                          metric = "logLoss",
                          tuneGrid = par_grid,
                          trControl = control_logloss)

# Ridge model without downsampling for water temperature
ridge_a1_logloss <- train(a1, 
                          data = training, 
                          method = "glmnet",
                          metric = "logLoss",
                          tuneGrid = par_grid,
                          trControl = control_logloss)
# Black magic
stopCluster(cl)

# Save models
write_rds(ridge_a1_logloss, "assessment/ridge_a1_logloss.rds")
write_rds(ridge_w1_logloss, "assessment/ridge_w1_logloss.rds")

# Remove models
rm(ridge_a1_logloss, ridge_w1_logloss)
```

New models with logloss and downsampling.

```{r: First ridge regression w/ downsampling}
# Black magic
par_grid <-  expand.grid(alpha = 0,
                        lambda = seq(3.5e-8, 3.5e7,
                                     length.out = 8))

# Black magic
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

# Sed seed
set.seed(284)

# Ridge model with downsampling for air temperature
ridge_a1_logloss_downsampled <- train(a1, 
                                      data = training, 
                                      method = "glmnet",
                                      metric = "logLoss",
                                      tuneGrid = par_grid,
                                      trControl = control_logloss_ds)

# Run 
logistic_w1 <- glm(w1, training, family = "binomial")


# Ridge model with downsampling for water temperature
ridge_w1_logloss_downsampled <- train(w1, 
                                      data = training, 
                                      method = "glmnet",
                                      metric = "logLoss",
                                      tuneGrid = par_grid,
                                      trControl = control_logloss_ds)

# Black magic
stopCluster(cl)

# Save models
write_rds(ridge_a1_logloss_ds, "assessment/ridge_a1_logloss_ds.rds")
write_rds(ridge_w1_logloss_ds, "assessment/ridge_w1_logloss_ds.rds")

# Remove models
rm(ridge_a1_logloss_ds, ridge_w1_logloss_ds)
```

### Step 7: Logistic regressions with random effects

```{r: First random effects logistic}
## Random effects model for water temperature
#w1_re <- summerkill ~ long + lat + season + variance_after_ice_30 + variance_after_ice_60 + log_schmidt +
#  cumulative_above_10 + ice_duration + water_pca + (1 | site_id)
#
## Black magic
#re_w1 <- glmer(w1_re,
#               data = training, 
#               family = binomial, 
#               control = glmerControl(optimizer = "bobyqa"),
#               nAGQ = 10)
#
#### Warning messages:
#### 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
####   unable to evaluate scaled gradient
#### 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
####    Hessian is numerically singular: parameters are not uniquely determined
#   
## Black magic (changed from lmer to glmer; removed unused argument (REML = FALSE))
#rf_w1 <- glmer(w1_re, 
#               data = training,
#               family = binomial,
#               control = glmerControl(optimizer = 'optimx',
#                                      optCtrl = list(method = 'L-BFGS-B')))
#
## Random effects model for air temperature
#a1_re <- summerkill ~ long + lat + season + air_pca + (1 | site_id)
#   
## Black magic
#re_a1 <- glmer(a1_re,
#               data = training, 
#               family = binomial, 
#               control = glmerControl(optimizer = "bobyqa"),
#               nAGQ = 10)
#
#
## Black magic (changed from lmer to glmer; removed unused argument (REML = FALSE))
#rf_a1 <- glmer(a1_re, 
#               data = training,
#               family = binomial,
#               control = glmerControl(optimizer = 'optimx',
#                                      optCtrl = list(method = 'L-BFGS-B')))
#
## Save models
#write_rds(re_w1, "assessment/re_w1.rds")
#write_rds(rf_w1, "assessment/rf_w1.rds")
#write_rds(re_a1, "assessment/re_a1.rds")
#write_rds(rf_a1, "assessment/rf_a1.rds")
#
## Remove models
#rm(re_w1, rf_w1,
#   re_a1, rf_a1)
```

######################################
Proceed to `04_assess_models.Rmd`
######################################
